<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="RJCP est une conf&eacute;rence organis&eacute;e par des doctorants pour des jeunes chercheurs (Master 2, doctorants, post-docs, ATER...)." />
	<meta name="keywords" content="rjcp, rjcp2009 avignon, rjcp 2009 avignon, stic, sciences technologie information communication, conf&eacute;rence, manifestation, jeune cherche, doctorant " />
	<meta name="robots" content="index, follow, all" />
	<meta name="language" content="fr" />

	<title>RJCP 2009 Avignon</title>
	
		<script type="text/javascript" src="js/mootools-1.2.1-core-yc.js"></script>
		<script type="text/javascript" src="js/mootools-1.2-more.js"></script>		
		
		<!--[if IE]>
		<script type="text/javascript" src="js/minmax.js"></script>
		<![endif]-->
		
		
		<script type="text/javascript">
		var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
		document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
		</script>
		<script type="text/javascript">
		try {
		var pageTracker = _gat._getTracker("UA-7573503-1");
		pageTracker._trackPageview();
		
		
		function changeFontSize(inc)
		{
			var elementsToChangeFontSize = new Array('body', 'h1', 'h2', 'h3', 'ul', 'li', 'p', 'th');
			for (i=0; i < elementsToChangeFontSize.length; i++) {
				var p = document.getElementsByTagName(elementsToChangeFontSize[i]);
				for(n=0; n<p.length; n++) {	
					if(p[n].style.fontSize) {
						var size = parseInt(p[n].style.fontSize.replace("px", ""));
						p[n].style.fontSize = size+inc + 'px';
					}
					else {
						
					} 
			   }
			}
		}
		
		} catch(err) {}</script>
	
		<link href="css/style.css" rel="stylesheet" type="text/css" media="screen" />

	</head>
<body>

<map name="map">
 <area shape="rect" coords="63,240,213,286" href="session_1.html" />
 <area shape="rect" coords="62,315,211,359" href="session_2.html" />
 <area shape="rect" coords="264,120,414,166" href="session_3.html" />
 <area shape="rect" coords="265,194,413,240" href="session_4.html" />
 <area shape="rect" coords="466,210,615,258" href="session_5.html" />
 <area shape="rect" coords="63,174,211,210" href="invites.html#jeanneret" />
 <area shape="rect" coords="264,316,362,354" href="invites.html#beaugendre" />
 <area shape="rect" coords="466,144,615,181" href="invites.html#rossato" />
</map>

  <!-- begin #wrapper_header -->
  <div id="wrapper_header">

    <!-- begin #header -->
	<div id="header">
		<a href="../index.html" class="simple_link">
			<img src="images/MajecSTIC2009.png" alt="Majecstic2009" title="RJCP 2009 &agrave; Avignon" id="date_conf" width="255px" />

		</a>
	</div>
    <!-- end #header -->


  <div id="wrapper">
    <div id="page">
    
      
		<div id="wrapper_sidebar">
			<div id="sidebar">
        
				<ul>
				<li><a href="index.html" class='simple_link'>Pr&eacute;sentation</a></li>
				<li><a href="edito.html" class='simple_link'>Message des organisateurs</a></li>
				<li><a href="comites.html" class='simple_link'>Comit&eacute;s</a></li>
				<li><a href="files/Programme_et_recueil_des_resumes_RJCP.pdf" class='simple_link'>Programme [PDF]</a></li>
				<li><a href="invites.html" class='simple_link'>Conf&eacute;rences invit&eacute;es</a></li>
				<li><a href="sessions.html" class='simple_link'>Index des sessions</a></li>
				<li><a href="topics.html" class='simple_link'>Index des th&eacute;matiques</a></li>
				<li><a href="auteurs.html" class='simple_link'>Index des auteurs</a></li>
				<li><a href="articles.html" class='simple_link'>Tous les articles</a></li>
				<li><a href="stats.html" class='simple_link'>Statistiques</a></li>
				<li><a href="logiciels/liens.html" class='simple_link'>Logiciels Audio</a></li>
				<li><a href="http://rjcp2009.univ-avignon.fr/actes.php" class='simple_link'>Actes en ligne</a></li>
				</ul>  
			</div>

			<div class="submenu">

			<div class="logo_nplia">
			<br/>Manifestation support&eacute;e par l'<strong>association <abbr title="Non Permanents du Laboratoire Informatique d'Avignon">NP-LIA</addr></strong><br/>

			<a href="http://www.np-lia.fr/" class="simple_link"><img src="images/np-lia_v_claire.png" alt="Non Permanents du Laboratoire Informatique d'Avignon" title="Non Permanents du Laboratoire Informatique d'Avignon" width="160px"/></a><br/>
			NP-LIA organise <a href="../MajecSTIC/index.html" class="simple_link">MajecSTIC</a> parallèlement aux <abbr title="Rencontres Jeunes Chercheurs en Parole">RJCP</abbr>.
			</div>

			</div>
		</div>
    
		<div id="content">

				<div class="post">
					<h2 class="title">Article 62</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Vers des Modèles Autonomes de Reconnaissance Automatique de la Parole  Multilingue</font></strong></li>					<li><strong>Sethserey Sam </strong> (Laboratoire d'Informatique de Grenoble (LIG))</li>
						<li><strong>R&eacute;sum&eacute; : </strong> In multilingual automatic speech recognition, one interesting research challenge is how to deal with a multilingual speech utterance (the utterance that contains different speech languages and/or native or non-native speech)? In order to overcome this problem, we focus our research on autonomous acoustic models (AM) and language models (LM). Autonomous means the multilingual AM and LM are automatically re-adapted themselves, in every given time slot (5s or 10s), before final decoding. The re-adaptation of AM and ML models could be done based on a module called Autonomous observer. In this article, we introduce the concept of autonomous AM and ML in multilingual ASR system (for automatic phone transcription purpose) and also the techniques to create an observer module</li>
						<li><a href="articles/62.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 72</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Utilisation d’une grille polaire adaptative pour la construction d’un modèle articulatoire de la langue</font></strong></li>					<li><strong>Julie Busset </strong> (LORIA)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> The construction of articulatory models from medical images of the vocal tract, especially X-ray images, relies on the application of an articulatory grid before deriving deformation modes via some factor analysis method. One difﬁculty faced with the classical semi-polar grid is that some tongue contours do not intersect the grid what gives rise to incomplete input vectors, and consequently poor tongue modeling in the front part of the mouth cavity which plays an important role in the articulation of many consonants. First, this paper describes preparation of data, i.e. drawing or tracking articulator contours, compensation of head movements and the construction of the adaptive polar grid. Then, the results of the principal component analysis are presented and compared with those obtained with the semi-polar grid.</li>
						<li><a href="articles/72.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 82</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">La densité des idées : un modèle d’analyse du discours pertinent pour le diagnostic précoce de la maladie d’Alzheimer ?</font></strong></li>					<li><strong>Hye Ran Lee </strong> (Laboratoire Praxling, UMR5267-CNRS/ Université Montpellier 3)</li>					<li><strong>Melissa Barkat-Defradas </strong> (Laboratoire Praxling, UMR5267-CNRS/ Université Montpellier 3)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> La dégradation linguistique est un indicateur précoce de la maladie d’Alzheimer (MA). Ainsi, l’étude du langage des patients atteints de MA peut être considérée comme un champ d’investigation prometteur pour l’établissement d’un diagnostic précoce de cette maladie. La présente étude examine les discours oraux de deux groupes : les patients diagnostiqués comme atteints de démence de type Alzheimer (DTA) et des sujets de contrôle. La densité des idées (DI) de chaque transcription a été calculée à l’aide de l’analyse prédicative. Les résultats montrent qu’il y a une différence significative entre ces deux groupes.</li>
						<li><a href="articles/82.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 92</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Perception d’expressions multimodales du Feeling of Thinking  (états mentaux et affectifs, intentions, attitudes) en interaction </font></strong></li>					<li><strong>Anne Vanpé </strong> (GIPSA-lab, Département Parole et Cognition (ex-ICP), UMR 5216 CNRS/Université de Grenoble)</li>					<li><strong>Véronique Aubergé </strong> (GIPSA-lab, Département Parole et Cognition (ex-ICP), UMR 5216 CNRS/Université de Grenoble)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Human-Machine Interaction, as interaction between two humans, can be considered as a dynamic process where the human is continuously communicating, even when he is “expressively” listening (informative backchannel and feedback). The present study analyses the audio-visual non speech expressions for two subjects in spontaneous HMI corpora, following an ethology-based methodology. First results reveal a large panel of values expressed outside of turns (e.g. mental states, intentions, attitudes, emotions) that we have globally called Feeling of Thinking. We have shown the role of static vs. dynamic processing of visual information and we are now attempting to investigate some specific non speech “vocal events”. Their temporal distribution seems to be particularly relevant for the perception of Feeling of Thinking expressions.</li>
						<li><a href="articles/92.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 102</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Traduction automatique de la parole arabe/anglais par segmentations multiples</font></strong></li>					<li><strong>Fethi Bougares </strong> (Laboratoire d'Informatique de Grenoble)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> La traduction de la parole est un thème de recherche récent, car il combine deux problèmes scientifiques complexes : la reconnaissance de la parole et la traduction automatique.  Dans ce contexte, nous nous intéressons à la construction de système de traduction statistique pour la paire de langues arabe/anglais.   Ces deux langues sont de structures éloignées, ce qui nécessite plus d'effort de préparation et de segmentation des données textuelles ou orales à traduire. Après avoir mis en lumière la relation entre l'analyse morphologique de l'arabe et la qualité de traduction, nous abordons les problèmes relatifs à l'ambiguïté segmentale de l'arabe avec la formulation et l'intégration de la multi-segmentation dans un système de traduction statistique.</li>
						<li><a href="articles/102.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 112</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Identification des consonnes finales du vietnamien par des locuteurs natifs</font></strong></li>					<li><strong>Thi-Thuy-Hien Tran </strong> (Département Parole et Cognition de GIPSA-lab)</li>					<li><strong>Nathalie Vallée </strong> (Département Parole et Cognition de GIPSA-lab)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> A great difficulty encountered by Vietnamese subjects, who learn French, is that consonant clusters, which do not exist in Vietnamese, are mispronounced. This problem persists even after several years of practicing, and even when the French clusters correspond to Vietnamese consonant sequences. The general aim of our project is to identify the factors which are the main cause of this problem. In this paper, we examine the perception of syllable-final stops (/p/, /t/, /k/, /m/, /n/, /&#x014B;/) in Vietnamese by 20 native Northern-Vietnamese listeners. Our findings suggest that specific acoustic characteristics and probably the lexical frequency of final consonants lead the subjects in their choice of responses.</li>
						<li><a href="articles/112.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 122</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Perception de la variation linguistique : étude comparative entre l’aire de Lesbos (Grèce) et celle des « vallées vaudoises » du Piémont occidental (Italie)</font></strong></li>					<li><strong>Silvia Gally </strong> (GIPSA-lab UMR 5216, DPC – SLD  Université Stendhal - Grenoble 3 )</li>					<li><strong>Maria Goudi </strong> (GIPSA-lab UMR 5216, DPC – SLD  Université Stendhal - Grenoble 3 )</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Dans cet article nous proposons une étude de dialectologie perceptuelle (DP) qui met en parallèle des travaux effectués dans deux aires linguistiques bien distinctes : l’île de Lesbos, en Grèce, et une zone du Piémont occidental, en Italie. Les données traitées dans ces études sont issues d’enquêtes de terrain dans les deux aires respectives.</li>
						<li><a href="articles/122.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 132</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Etude articulatoire du mouvement d’étirement et d’ouverture des lèvres lors d’émotions et une attitude simulées.</font></strong></li>					<li><strong>Laurianne Georgeton </strong> (ilpga)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Dans cette étude, nous nous sommes intéressée aux variations articulatoires des lèvres (étirement et ouverture aux lèvres) pour quatre voyelles du français /a/, /i/, /u/ et /y/ lors d'émotions (anxiété, dégoût, colère, joie et tristesse) et une attitude (tendresse) simulée. Nous avons utilisé des marqueurs placés autours des lèvres. Le mouvement des lèvres a été étudié grâce au Qualisys qui utilisent un système de caméra infra-rouge. Les coordonnées en 3D des marqueurs sont ensuite reconstruites puis étudiées. Cette étude a montré que les contrastes intrinsèques (donc attendus) des voyelles ne sont pas observables lors de la parole normale. Le geste d'étirement est maximisé lors de la réalisation de la joie et de la tendresse. Le geste d'ouverture aux lèvres est maximisé lors de la colère, du dégoût et de l'anxiété.</li>
						<li><a href="articles/132.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 142</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Etude perceptive et articulatoire à partir de données échographiques de la langue chez des patients hémiglossectomisés </font></strong></li>					<li><strong>Audrey Acher </strong> (Laboratoire de Phonétique et Phonologie UMR 7018 Paris)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Le but de cette étude est d'évaluer la récupération de l’articulation après hémiglossectomie avec reconstruction par lambeau infra-hyoïdien chez deux patients. Lors de cette étude longitudinale nous avons réalisé une évaluation perceptive et une évaluation articulatoire à l’aide d’un échographe portable. L’analyse statistique des données perceptives a montré que les consonnes occlusives vélaires, les fricatives apico-alvéolaires, le /l/ et le /j/ sont les plus altérés après hémiglossectomie. Ces données révèlent une amélioration significative de la perception de l’articulation trois mois après l’intervention chez les deux patients. L’évaluation de la mobilité des articulateurs et l’analyse des contours linguaux des consonnes /k/, /g/, /s/ et /z/ ne mettent pas en évidence la récupération des capacités fonctionnelles chez les patients à ce délai.</li>
						<li><a href="articles/142.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 152</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Représentations cérébrales des articulateurs de la parole</font></strong></li>					<li><strong>Krystyna Grabski </strong> (Gipsa-Lab)</li>					<li><strong>Marc Sato </strong> (Gipsa-Lab)</li>					<li><strong>Jean-Luc Schwartz </strong> (Gipsa-Lab)</li>					<li><strong>Laurent Lamalle </strong> (INSERM)</li>					<li><strong>Coriandre Vilain </strong> (Gipsa-Lab)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> In order to localize cerebral regions involved in articulatory control processes, ten subjects were examined using functional magnetic resonance imaging while executing lip, tongue and jaw movements. Although the three motor tasks activated a set of common brain areas classically involved in motor control, distinct movement representation sites were found in the motor cortex. These results support and extend previous brain imaging studies by demonstrating a sequential dorsoventral somatotopic organization of lips, jaw and tongue in the motor cortex. </li>
						<li><a href="articles/152.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 162</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Peut-on utiliser la voix chantée pour améliorer la correction phonétique segmentale en langue étrangère ?</font></strong></li>					<li><strong>Sandra Cornaz </strong> (GIPSA-Lab, DPC - Département Parole et Cognition - UMR 5216 CNRS/Université de Grenoble)</li>					<li><strong>Nathalie  Henrich </strong> (GIPSA-Lab, DPC - Département Parole et Cognition - UMR 5216 CNRS/Université de Grenoble)</li>					<li><strong>Antonio Romano </strong> (LPEAG, Laboratorio di Fonetica Sperimentale “Arturo Genre” di Torino, Italie)</li>					<li><strong>Nathalie Vallée </strong> (GIPSA-Lab, DPC - Département Parole et Cognition - UMR 5216 CNRS/Université de Grenoble)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Music may have a positive impact on learning processes. In linguistics, the positive role of music on perception of prosodic features has been pointed out, and a recent study shows that the segmentation of words in a foreign language would be facilitated by sung. In the present study, we aim at investigating whether singing-voice tasks could help to improve the learning of French phonemes. For comparison purpose, a traditional phonetic method was slightly modified to introduce singing-voice tasks. Native speakers of Italian were divided into two groups: one for the common phonetic teaching, and one for the phonetic teaching including singing-voice tasks. The results show that the subjects who were taught with additional singing-voice tasks learn faster than the others, produce better than the other one the anterior phonemes /y/ and /ø/ in the acoustical regions expected for these vowels in French, and the overlap of acoustic scatterings is less important.</li>
						<li><a href="articles/162.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 172</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Architecture d’un Système de Vérification Automatique du Locuteur appuyée par la Détection du Genre</font></strong></li>					<li><strong>Hayet Djellali </strong> (Université de Badji Mokhtar Annaba, Algérie)</li>					<li><strong>Radia Amirouche </strong> (Université de Badji Mokhtar Annaba, Algérie)</li>					<li><strong>Mohamed Tayeb Laskri </strong> (Universite de badji mokhtar Annaba, Algérie)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> We propose a new approach in Automatic speaker verification ASV based on detection Gender (male,female). We determine with speaker voice his gender. Knowing that, the speaker could be an impostor with opposite gender that he claims. The aim of this work is to experiment if detection gender module can improve speaker verification decision when we compare it with baseline ASV system.</li>
						<li><a href="articles/172.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 182</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">HMMs and GMMs based methods in acoustic-to-articulatory speech inversion</font></strong></li>					<li><strong>Atef Ben Youssef </strong> (DPC / GIPSA-lab, UMR 5216, Grenoble)</li>					<li><strong>Viet-Ahn Tran </strong> (DPC / GIPSA-lab, UMR 5216, Grenoble)</li>					<li><strong>Pierre Badin </strong> (DPC / GIPSA-lab, UMR 5216, Grenoble)</li>					<li><strong>Gérard Bailly </strong> (DPC / GIPSA-lab, UMR 5216, Grenoble)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Afin de récupérer les mouvements des articulateurs tels que les lèvres, la mâchoire ou la langue, nous avons développé et comparé deux méthodes d’inversion basées l’une sur les modèles de Markov cachés (HMMs) et l’autre sur les modèles de mélanges de gaussiennes (GMMs). Les mouvements des articulateurs sont représentés par les coordonnées médiosagittale de bobines d’un articulographe électromagnétique (EMA) fixées sur les articulateurs. Dans la première méthode, des HMMs à deux flux, acoustique et articulatoire, sont entrainés à partir des de signaux acoustique et articulatoire synchrones. Le HMM acoustique sert à reconnaitre les phones, ainsi que leurs durées. Ces informations sont ensuite utilisées par le HMM articulatoire pour synthétiser les trajectoires articulatoires. Pour la deuxième méthode, un GMM s’association entre traits acoustique et articulatoire est entrainé sur le même corpus suivant le critère de minumum d’erreur quadratique moyenne (MMSE) à partir des trames acoustiques d’empan temporel plus ou moins grand. Pour un corpus de données EMA mono-locuteur enregistré par un locuteur français, l’erreur RMS de reconstruction sur le corpus de test pour la méthode fondée sur les HMMs se situe entre 1.96 et 2.32 mm, tandis qu’elle se situe entre 2.46 et 2.95 mm pour la méthode basé sur les GMMs.</li>
						<li><a href="articles/182.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 202</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Rôle de l’information visuelle dans l’accès au lexique mental</font></strong></li>					<li><strong>Mathilde Fort </strong> (Laboratoire de Psychologie et de Neurocognition)</li>					<li><strong>Justine Chipot </strong> (Laboratoire de Psychologie et de Neurocognition)</li>					<li><strong>Sonia Kandel </strong> (Laboratoire de Psychologie et de Neurocognition)</li>					<li><strong>Christophe Savariaux </strong> (GIPSA-Lab)</li>					<li><strong>Elsa Spinelli </strong> (Laboratoire de Psychologie et de Neurocognition)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Cette étude vise à déterminer le rôle de l'information visuelle dans l'accès au lexique.Pour cela,nous avons utilisé un paradigme d'amorçage phonologique. Les participants devaient effectuer une tâche de décision lexicale sur une cible présentée en modalité auditive.Cette cible était toujours précédée par une syllabe en amorce: cette dernière pouvait être présentée en modalité audiovisuelle (AV), auditive (A), ou visuelle seule (V). L'analyse des résultats sur les mots cibles indique un effet d'amorçage pour toutes les modalités de présentations de la syllabe. En conséquence, notre étude suggère que l'information visuelle seule permet d'activer les représentations de mots contenues dans le lexique mental.</li>
						<li><a href="articles/202.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 212</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Corrections spécifiques du français sur les systèmes de reconnaissance automatique de la parole</font></strong></li>					<li><strong>Richard Dufour </strong> (LIUM - Université du Maine)</li>					<li><strong>Yannick Estève </strong> (LIUM - Université du Maine)</li>					<li><strong>Paul Deléglise </strong> (LIUM - Université du Maine)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Automatic speech recognition (ASR) systems are used in a large number of applications, in spite of the inevitable recognition errors.  In this study we propose a pragmatic approach to automatically repair ASR outputs by taking into account linguistic and acoustic information, using formal rules or stochastic methods. The proposed strategy consists in developing a specific correction solution for each specific kind of errors.  In this paper, we apply this strategy on two case studies specific to French language.  We show that it is possible, on automatic transcriptions of French broadcast news, to decrease the error rate of a specific error by 11.4% in one of two the case studies, and 86.4% in the other one. These results are encouraging and show the interest of developing more specific solutions to cover a wider set of errors in a future work.</li>
						<li><a href="articles/212.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 222</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Caractérisation automatique des accents étrangers</font></strong></li>					<li><strong>Abdelkarim Mars </strong> (Laboratoire d'informatique de grenoble)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Parmi les phénomènes qui affectent la manière dont nous parlons, l’accent est une des composantes principales de la variation observée. La prononciation d’un locuteur peut en effet nous renseigner sur son origine, géographique et sociale. La description des caractéristiques phonétiques qui sous-tendent les différences d’accent perçues constitue donc un intérêt scientifique particulier. De plus, la recherche dans le domaine des accents contribue a l’amélioration d’applications technologiques telles que la reconnaissance de la parole et l’indexation du locuteur.  Ce papier propose une étude phonétique acoustique des accents étrangers en français. Afin d’analyser à grande échelle les variations liées a l’origine de locuteur, nous avons évalue l’apport des outils automatiques décodage acoustico-phonétique et alignement force.</li>
						<li><a href="articles/222.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 232</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Une Base de données Etiquetée Formantiquement en Langue Arabe Standard</font></strong></li>					<li><strong>Imen Jemaa </strong> (Unité de Recherche Traiement du Signal, Traitement de l'image et Reconnaissance de Formes, Tunisie)</li>					<li><strong>Oussama Rekhis </strong> (Unité de Recherche Traiement du Signal, Traitement de l'image et Reconnaissance de Formes, Tunisie)</li>					<li><strong>Kais Ouni </strong> (Unité de Recherche Traiement du Signal, Traitement de l'image et Reconnaissance de Formes, Tunisie)</li>					<li><strong>Yves Laprie </strong> (Equipe Parole, LORIA Nancy1, France)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> While formant frequencies are known to play a critical role in human speech perception and in computer speech processing, there has been a lack of standard databases needed for the quantitative evaluation of automatic formant extraction techniques especially in Arabic language. We report in this paper our recent effort to create a reference database of the first three formant tracks. The manually Formant labeling is carried out used the Winsnoori tool. Furthermore, we present in this paper an exploratory use of the database to quantitatively evaluate the automatic LPC method implemented in the popular open source Praat using the hand edited formant trajectories as reference. </li>
						<li><a href="articles/232.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 242</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Construction d’un corpus robuste de différents dialectes arabes</font></strong></li>					<li><strong>Mohamed Belgacem </strong> (Laboratoire LIDILEM )</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Notre article s’intègre dans le cadre du projet intitulé 'Oréodule' : un système embarqué temps réel de reconnaissance, de traduction et de synthèse de la parole arabe. L’objet de notre intérêt dans cet article est la présentation d’un corpus vocal de la parole arabe. Nous détaillerons les étapes de constitution de ce corpus et les difficultés rencontrées lors de son élaboration. Nous intègrerons également les différents résultats pratiques obtenus lors de chaque phase (tailles des enregistrements, volume total du notre corpus, etc.).</li>
						<li><a href="articles/242.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 252</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Espace perceptuel de similarité : étude sur 17 langues</font></strong></li>					<li><strong>Marie Rimbault Joffard </strong> ()</li>
						<li><strong>R&eacute;sum&eacute; : </strong> The goal of the present study was to device a means of representing languages in a perceptual similarity space based on their overall sound structures. In experiment 1, native French listeners performed a free classification task in which they grouped 17 diverse languages based on their overall similarity. A similarity matrix of the grouping patterns was then submitted to clustering and multidimensional scaling analyses. In experiment 2, the same group of French listeners sorted the 17 languages in term of their distance to French. Taken together, the results of the two experiments provide the basis for estimating the distance between a given mother tongue and other languages and for understanding the role of the phonological filter.</li>
						<li><a href="articles/252.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 262</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Étude descriptive préliminaire de la voix de l'enfant implanté cochléaire à partir des mesures aérodynamiques </font></strong></li>					<li><strong>Harold Andrés Guerrero Lopez </strong> (Praxiling UMR 5267 CNRS - Montpellier III)</li>					<li><strong>Benoit Amy De La Breteque </strong> (CHU Gui de Chauliac, Montpellier)</li>					<li><strong>Michel Mondain </strong> (CHU Gui de Chauliac, Montpellier)</li>					<li><strong>Patrick Serrafero </strong> (Ecole Centrale de Lyon)</li>					<li><strong>Catherine  Trottier </strong> (UMR I3M - Université Montpellier II)</li>					<li><strong>Melissa  Barkat-Defradas </strong> (Praxiling UMR 5267 CNRS - Montpellier III)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> The purpose of this study was to describe the voice physiological characteristics of cochlear implanted children by voice aerodynamic measurements. Subjects were 6 girls and 14 boys prelingual or congenital profound deaf children. Voice aerodynamic measurements were obtained from the children by EVA™2 system : estimated subglotic pressure (PSGE), oral airflow mean (DAB), intensity, glottal efficiency, laryngeal efficiency. Although our findings are descriptive and not have been compared to other populations at the present study (such as children with normal hearing and hearing aids), we can assume that cochlear implanted children's voice physiological behavior is similar to the phonatory behavior of children with normal hearing.</li>
						<li><a href="articles/262.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 272</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Modélisation Stochastique du Dialogue par Structures Sémantiques</font></strong></li>					<li><strong>Florian Pinault </strong> (CERI-LIA)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Dans le domaine de l'interaction Homme-Machine, les systèmes de dialogue à initiative mixte sont actuellement étudiés, afin de permettre aux utilisateurs de parler librement avec la machine. Cependant, les système de dialogue en langue naturelle manque souvent de la robustesse nécessaire pour assurer la satisfaction de l'utilisateur. Une solution consiste à utiliser une représentation sémantique riche du dialogue, ainsi qu'une modélisation statistique du cours du dialogue.</li>
						<li><a href="articles/272.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
				<div class="post">
					<h2 class="title">Article 282</h2>
					<div class="entry">
					<ul>
						<li><strong>Titre : <font color="red">Méthodes objectives issues du traitement automatique de la parole pour la recherche de zones 'déviantes' dans la parole dysarthrique</font></strong></li>					<li><strong>Pierre Clement </strong> (Laboratoire Informatique d'Avignon)</li>					<li><strong>Corinne Fredouille </strong> (Laboratoire Informatique d'Avignon)</li>
						<li><strong>R&eacute;sum&eacute; : </strong> Une déficience ou un dysfonctionnement d’une enzyme présente dans les lysosomes est à l’origine des maladies de surcharge lysosomale (ou maladies lysosomales). Parmi les nombreux symptômes pouvant être liés à ces maladies, les patients peuvent être atteints de dysarthrie. La dysarthrie se définit par un trouble de l’élocution dû à une lésion du système nerveux. A l’heure actuelle, l’évaluation du degré de sévérité de la dysarthrie se fait de façon perceptive par les cliniciens. Bien qu’il existe des critères perceptuels et visuels définis sur lesquels les cliniciens peuvent s’appuyer pour évaluer la dysarthrie, cette évaluation reste très dépendante du clinicien l’effectuant, et revêt par conséquent un caractère très subjectif. Pour cette raison, la mise en place de méthodes plus objectives de l’évaluation de la dysarthrie devient une nécessité. Cette mise en place doit reposer au préalable sur une meilleure connaissance et compréhension des phénomènes acoustico-phonétiques liés à la parole dysarthrique.  Cet article décrit les méthodologies objective mise en place afin de rechercher des zones 'déviantes' dans la parole dysarthrique. Cette analyse de la parole dysarthrique sera effectuée grâce à des outils de traitement automatique de la parole.</li>
						<li><a href="articles/282.pdf">Voir l'article entier</a></li>
					</ul>
					</div>
				</div>
	
	
			
			
		</div>
		
		
	
		<!-- end #sidebar -->
		<div style="clear: both;">&nbsp;</div>
	</div>

	<!-- end #page -->
</div>


	<div id="footer">
    

		<p><a href="http://www.univ-avignon.fr" title="Universit&eacute; d'Avignon et des Pays de Vaucluse">Universit&eacute; d'Avignon</a> 
		| 
		<a href="http://www.lia.univ-avignon.fr" title="Laboratoire Informatique d'Avignon">Laboratoire Informatique d'Avignon</a>
		| 
		<a href="http://www.univ-avignon.fr/fr/recherche/laboratoires/strlab/structure/laboratoire-culture-et-communication-ea-3151.html" title="Laboratoire Culture et Communication">Laboratoire Culture et Communication</a>
		</p>
		<p>
		<!--
		  <a href="http://www.univ-avignon.fr" class="simple_link" title="Universit&eacute; d'Avignon et des Pays de Vaucluse"><img src="./images/logo_uapv.png" /></a> <a href="http://www.lia.univ-avignon.fr"   class="simple_link" title="Laboratoire Informatique d'Avignon"><img src="./images/logo_lia.png" /></a> <a href="http://www.univ-avignon.fr/fr/recherche/laboratoires/strlab/structure/laboratoire-culture-et-communication-ea-3151.html"><img src="./images/logo_lcc.png" /></a>
		  -->
		  
		<a href="http://www.univ-avignon.fr" class="simple_link" title="Universit&eacute; d'Avignon et des Pays de Vaucluse"><img src="images/logo_uapv.png" /></a>
		<a href="http://www.lia.univ-avignon.fr" class="simple_link" title="Laboratoire Informatique d'Avignon"><img src="images/logo_lia.png" /></a>
		<a href="http://www.edi2s.univ-montp2.fr/" class="simple_link" title="Ecole Doctorale Information Structures Systèmes"><img src="images/i2s.png" /></a>
		<br/>

		<a href="http://www.univ-avignon.fr/fr/recherche/laboratoires/strlab/structure/laboratoire-culture-et-communication-ea-3151.html" class="simple_link" title="Laboratoire Culture et Communication"><img src="images/logo_lcc.png" /> </a>
		</p>
			</div>

	<!-- end #footer -->
  </div>

</body>
</html>
